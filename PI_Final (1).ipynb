{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio # inspatalação dos recursos necessários para criar uma interface visual\n",
        "nltk.download('stopwords') #garante que as stopwords estejam disponíveis localmente."
      ],
      "metadata": {
        "id": "oyjssCSP9FeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjJHNUCE6jTE"
      },
      "outputs": [],
      "source": [
        "# importaççoes das bibliotecas necessárias para manipulação de dados, pré-processamento de texto, interface e vetorização...\n",
        "import pandas as pd # manipulação de dados\n",
        "import re #pré-processamento de texto\n",
        "import unicodedata #pré-processamento de texto\n",
        "import nltk #pré-processamento de texto\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer #vetorização de texto\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #vetorização de texto\n",
        "import gradio as gr # interface interativa\n",
        "from sklearn.model_selection import train_test_split # Divisão dos dados para treinamento e Teste\n",
        "from sklearn.linear_model import LogisticRegression # Importação da biblioteca para LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score # Métricas para ver a performance do modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_treinamento = pd.read_excel('Base_Pedidos.xlsx', engine='openpyxl') # Carregamento dos dados de pedidos no df de treinamento\n",
        "df_treinamento['request_normalized'] = df_treinamento['request'] #criação de uma coluna para armazenar versões normalizadas das descrições de pedidos."
      ],
      "metadata": {
        "id": "PVeY7kyH6n46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicações prévias:\n",
        "O método .apply() do pandas permite aplicar uma função em cada elemento de uma coluna ou DataFrame."
      ],
      "metadata": {
        "id": "Mp8V8hvhtied"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação de um dicionário para mapeamento de caracteres\n",
        "acentos_para_sem_acento = {\n",
        "    'á': 'a', 'à': 'a', 'ã': 'a', 'â': 'a', 'ä': 'a',\n",
        "    'é': 'e', 'è': 'e', 'ê': 'e', 'ë': 'e',\n",
        "    'í': 'i', 'ì': 'i', 'î': 'i', 'ï': 'i',\n",
        "    'ó': 'o', 'ò': 'o', 'õ': 'o', 'ô': 'o', 'ö': 'o',\n",
        "    'ú': 'u', 'ù': 'u', 'û': 'u', 'ü': 'u',\n",
        "    'ç': 'c',\n",
        "    'Á': 'A', 'À': 'A', 'Ã': 'A', 'Â': 'A', 'Ä': 'A',\n",
        "    'É': 'E', 'È': 'E', 'Ê': 'E', 'Ë': 'E',\n",
        "    'Í': 'I', 'Ì': 'I', 'Î': 'I', 'Ï': 'I',\n",
        "    'Ó': 'O', 'Ò': 'O', 'Õ': 'O', 'Ô': 'O', 'Ö': 'O',\n",
        "    'Ú': 'U', 'Ù': 'U', 'Û': 'U', 'Ü': 'U',\n",
        "    'Ç': 'C'\n",
        "}\n",
        "\n",
        "# Substituição dos caracteres que possuem acento, por caracteres sem acento\n",
        "def remover_acentos(texto):\n",
        "    texto_sem_acento = \"\"\n",
        "    for char in texto:\n",
        "        if char in acentos_para_sem_acento:\n",
        "            texto_sem_acento += acentos_para_sem_acento[char]\n",
        "        else:\n",
        "            texto_sem_acento += char\n",
        "    return texto_sem_acento\n",
        "\n",
        "# Aplicação na coluna df_treinamento['request_normalized'] que eliminará caracteres epeciais, do texto em minusculo e sem acentos\n",
        "df_treinamento['request_normalized'] = df_treinamento['request_normalized'].apply(\n",
        "    lambda x: re.sub(r'[^a-z0-9\\s]', '', remover_acentos(x).lower())\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "p-laO-om8AIH",
        "outputId": "ce4a9083-2fa8-4b89-89bd-4725f9df4aed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_treinamento' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9b6a2f668d11>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexto_sem_acento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m df_treinamento['request_normalized'] = df_treinamento['request_normalized'].apply(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-z0-9\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremover_acentos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_treinamento' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# depois de previamente tratados, aplicamos o split em cada palavra do texto\n",
        "df_treinamento['request_normalized'] = df_treinamento['request_normalized'].apply(lambda x: x.split())"
      ],
      "metadata": {
        "id": "DHvkl3sW8H3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação de um conjunto personalizado de stopwords para retirarmos as palavras desnecessárias\n",
        "stop_words_manual = {\n",
        "    'de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'e',\n",
        "    'com', 'nao', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as',\n",
        "    'dos', 'como', 'mas', 'foi', 'ao', 'ele', 'das', 'tem', 'a', 'seu',\n",
        "    'sua', 'ou', 'ser', 'quando', 'muito', 'ha', 'nos', 'ja', 'esta',\n",
        "    'eu', 'também', 'so', 'pelo', 'pela', 'ate', 'isso', 'ela', 'entre'\n",
        "}\n",
        "\n",
        "# retorna somente as palavras que não estão no conjunto de stopwords\n",
        "def remove_stopwords(words):\n",
        "    return [word for word in words if word not in stop_words_manual]\n",
        "\n",
        "# Aplicação da função remove_stopwords na coluna de requisições normalizadas\n",
        "df_treinamento['request_normalized'] = df_treinamento['request_normalized'].apply(remove_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "collapsed": true,
        "id": "3cK0dYbo8L4T",
        "outputId": "4bee729f-a630-4323-80ba-7d377f974542"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_treinamento' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7e8cd17fbcfc>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words_manual\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_treinamento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_normalized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_treinamento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_normalized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_treinamento' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Depois de tratarmos o texto, retirando caracteres especiais, deixando tudo minusculo e retirando as stopwords, ralizamos a união do texto. O lambda transforma essa lista em uma string única, com as palavras unidas por um espaço (' ') entre elas\n",
        "df_treinamento['request_normalized_str'] = df_treinamento['request_normalized'].apply(lambda x: ' '.join(x)) # x é cada elemento da coluna request_normalized\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=1000) #O parâmetro max_features=1000 limita o número máximo de palavras (ou \"features\") que serão consideradas no vocabulário. Apenas as 1.000 palavras mais frequentes nos dados serão incluídas no modelo. Isso é útil porque textos reais muitas vezes possuem milhares ou até milhões de palavras.\n",
        "vector = vectorizer.fit_transform(df_treinamento['request_normalized_str']) # temos a criação de uma matriz esparsa\n",
        "X = pd.DataFrame.sparse.from_spmatrix(vector, columns=vectorizer.get_feature_names_out()) # conversão de uma matriz esparsa em uma matriz de zeros e uns que indicam a recorrência de aparição de uma palavra em um texto\n",
        "\n",
        "y = df_treinamento['truthfulness'] # a catalogação prévia dos textos com 0 e 1 é utilizada como o eixo 1 para o treinamento\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Divisão do texto em conjunto de texto e de teste, o que pe o random_state\n",
        "\n",
        "# Métricas dos conjuntos\n",
        "print(\"Dimensões do conjunto de treino:\", X_train.shape)\n",
        "print(\"Dimensões do conjunto de teste:\", X_test.shape)\n",
        "print(\"Rótulos de treino:\", y_train.shape)\n",
        "print(\"Rótulos de teste:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "Id0zntFa8QLn",
        "outputId": "bc6bbb7d-9ec5-4cdd-dd47-a7c954c3ce06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_treinamento' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9f07426f39b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_treinamento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_normalized_str'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_treinamento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_normalized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_treinamento\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_normalized_str'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_spmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_treinamento' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinamento do modelo de regressão logística com os dados de treino, usado para prever categorias ou classes\n",
        "modelo = LogisticRegression(max_iter=1000, random_state=42) # max_inter = Este parâmetro define o número máximo de iterações que o algoritmo de otimização (como o gradiente descendente) pode realizar antes de parar.\n",
        "# random_state = Esse parâmetro define o valor da semente aleatória para garantir que os resultados sejam reproduzíveis\n",
        "\n",
        "#tudo isso foi feito para preparar o modelo para ser treinasdo\n",
        "\n",
        "modelo.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "collapsed": true,
        "id": "QDsklKyr8rz2",
        "outputId": "9ece143e-3692-4862-c489-0cae7dd66bec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LogisticRegression' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8d3909a41c57>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = modelo.predict(X_test)\n",
        "\n",
        "#Métricas de para avaliar a performance do modelo\n",
        "print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "1217f7kL8zoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remover_acentos(texto):\n",
        "    nfkd = unicodedata.normalize('NFKD', str(texto))\n",
        "    return \"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
        "\n",
        "def preprocessar_texto(texto):\n",
        "    texto = remover_acentos(texto.lower())\n",
        "    texto = re.sub(r'[^a-z0-9\\s]', '', texto)\n",
        "    palavras = texto.split()\n",
        "    palavras = [word for word in palavras if word not in stop_words_manual]\n",
        "    return ' '.join(palavras)\n",
        "\n",
        "def classificar_frase(frase):\n",
        "    frase_preprocessada = preprocessar_texto(frase)\n",
        "    frase_vectorizada = vectorizer.transform([frase_preprocessada])\n",
        "    previsao = modelo.predict(frase_vectorizada)\n",
        "\n",
        "    if previsao[0] == 0:\n",
        "        return \"0 - Pedido considerado falso\"\n",
        "    else:\n",
        "        return \"1 - O pedido parece verídico\"\n",
        "\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=classificar_frase,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Digite a descrição do pedido aqui...\", label=\"Descrição do Pedido\"),\n",
        "    outputs=gr.Textbox(label=\"Classificação\"),\n",
        "    title=\"Classificador de Pedidos\",\n",
        "    description=\"Digite uma descrição para verificar se o pedido é verdadeiro ou falso.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "o34MRSdN83tH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}